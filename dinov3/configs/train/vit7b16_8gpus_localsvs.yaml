dino:
  head_n_prototypes: 262144
  head_bottleneck_dim: 1024
  head_hidden_dim: 16384
  kde_loss_weight: 0.05 # 0.05
  koleo_loss_weight: 0.0 # 0 = no Ko Leo regularization
ibot:
  separate_head: true
  head_n_prototypes: 98304
  head_bottleneck_dim: 768
  head_hidden_dim: 8192
gram:
  use_loss: false
  # ckpt: ./checkpoints/dinov3_vit7b16_saved_teacher.pth
  # loss_weight: 0.02
  # it_first_update: 5000
  # it_load_ema_teacher: 5000
  # loss_weight_schedule:
  #   start: 0.0
  #   peak: 0.02
  #   end: 0.02
  #   warmup_epochs: 5
train:
  batch_size_per_gpu: 32
  num_workers: 10
  OFFICIAL_EPOCH_LENGTH: 1250
  centering: sinkhorn_knopp
  eval_and_ckpt_at_step0: true
  cache_dataset: true
  streaming_from_hf: false
  streaming_dataset:
    path: /data/TCGA_parquet_sample30_shuffled
    shuffle_buffer: 50000
    base_seed: 42
  dataset_path: "pathology:root=/data/TCGA/" # only used if streaming_from_hf is false
  checkpointing: true # enable to save memory
  checkpointing_full: false  # requires checkpointing also be true; more aggressive checkpointing
student:
  arch: vit_7b
  patch_size: 16
  drop_path_rate: 0.4
  layerscale: 1.0e-05
  ffn_layer: swiglu64
  ffn_ratio: 3.0
  norm_layer: layernormbf16
  n_storage_tokens: 4
  qkv_bias: false
  proj_bias: true
  ffn_bias: true
  mask_k_bias: true
  untie_global_and_local_cls_norm: true
  pos_embed_rope_rescale_coords: 2
  pos_embed_rope_dtype: fp32
  resume_from_teacher_chkpt: ./checkpoints/dinov3_vit7b16_saved_teacher.pth
  fp8_enabled: true # further save memory with fp8
teacher:
  momentum_teacher: 0.994
  final_momentum_teacher: 1.0
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 10 # 30
  in_chans: 3
optim:
  epochs: 8000
  early_stop: 400 #200
  lr: .0002
  weight_decay_end: 0.2
  warmup_epochs: 10 # 30
  clip_grad: 3.0
  layerwise_decay: 1.0
  patch_embed_lr_mult: 0.2
crops:
  use_pathology_hed: true
  gram_teacher_crops_size: 224
evaluation:
  eval_period_iterations: 2500 # save eval ckpt every x iterations (to be used for downstream)
checkpointing:
  allow_resume: true # false means to not ever save training state (so resuming will not work)
  period: 2500 # save training state every x iterations (to resume interrupted training runs)
  keep_every: 12500
logging:
  use_wandb: true
  project: path-fm-dinov3
  entity: null
  group: null
  job_type: null
  run_name: null
  tags: []
  artifact_logging: true
